#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import torch
import numpy as np
import pandas as pd
from pathlib import Path
import json
import time
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_loader import BrainNetworkDataset, create_data_loaders
from models import get_model
from trainer import BrainNetworkTrainer, GraphVAETrainer, GraphDiffusionTrainer


class BrainNetworkExperiment:
    """
    è„‘ç½‘ç»œæ•°æ®é¢„æµ‹å®éªŒä¸»ç±»
    """
    
    def __init__(self, 
                 config: Dict[str, Any],
                 output_dir: str = './experiments'):
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®éšæœºç§å­
        self.set_seed(config.get('seed', 42))
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç»“æœå­˜å‚¨
        self.results = {}
        
    def set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("åŠ è½½æ•°æ®...")
        
        # åˆ›å»ºæ•°æ®é›†
        self.dataset = BrainNetworkDataset(
            root=self.config['data']['root'],
            name=self.config['data']['name'],
            threshold=self.config['data']['threshold']
        )
        
        print(f"æ•°æ®é›†å¤§å°: {len(self.dataset)}")
        if len(self.dataset) > 0:
            print(f"èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {self.dataset[0].x.shape[1]}")
            # æ ¹æ®æ•°æ®é›†ç±»å‹æ˜¾ç¤ºä¸åŒä¿¡æ¯
            if self.dataset.name in ['HCPFI', 'HCPWM']:
                print(f"ä»»åŠ¡ç±»å‹: å›å½’")
                print(f"è¾“å‡ºç»´åº¦: 1")
            else:
                all_labels = torch.cat([data.y for data in self.dataset])
                print(f"ä»»åŠ¡ç±»å‹: åˆ†ç±»")
                print(f"ç±»åˆ«æ•°: {len(torch.unique(all_labels))}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader, self.test_loader, self.data_info = create_data_loaders(
            self.dataset,
            batch_size=self.config['training']['batch_size'],
            test_size=self.config['data']['test_size'],
            val_size=self.config['data']['val_size'],
            random_state=self.config['seed']
        )
        
        print(f"è®­ç»ƒé›†å¤§å°: {self.data_info['train_size']}")
        print(f"éªŒè¯é›†å¤§å°: {self.data_info['val_size']}")
        print(f"æµ‹è¯•é›†å¤§å°: {self.data_info['test_size']}")
        
    def run_single_experiment(self, model_name: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ¨¡å‹å®éªŒ"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å®éªŒ: {model_name.upper()}")
        print(f"{'='*60}")
        
        # åˆ›å»ºæ¨¡å‹
        model = get_model(
            model_name=model_name,
            input_dim=self.data_info['num_features'],
            num_classes=self.data_info['num_classes'],
            task_type=self.data_info['task_type'],
            **self.config['models'][model_name]
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        if model_name == 'graphvae':
            trainer = GraphVAETrainer(
                model=model,
                device=self.device,
                log_dir=self.output_dir / f'{model_name}_logs',
                save_dir=self.output_dir / f'{model_name}_checkpoints'
            )
        elif model_name == 'graphdiffusion':
            trainer = GraphDiffusionTrainer(
                model=model,
                device=self.device,
                log_dir=self.output_dir / f'{model_name}_logs',
                save_dir=self.output_dir / f'{model_name}_checkpoints'
            )
        else:
            trainer = BrainNetworkTrainer(
                model=model,
                device=self.device,
                log_dir=self.output_dir / f'{model_name}_logs',
                save_dir=self.output_dir / f'{model_name}_checkpoints'
            )
        
        # è®­ç»ƒæ¨¡å‹
        start_time = time.time()
        
        if model_name == 'graphdiffusion':
            # æ‰©æ•£æ¨¡å‹éœ€è¦å…ˆé¢„è®­ç»ƒ
            pretrain_results = trainer.pretrain_diffusion(
                self.train_loader,
                num_epochs=self.config['training']['pretrain_epochs'],
                learning_rate=self.config['training']['learning_rate']
            )
        
        # æ­£å¸¸è®­ç»ƒ
        train_results = trainer.train(
            train_loader=self.train_loader,
            val_loader=self.val_loader,
            num_epochs=self.config['training']['num_epochs'],
            learning_rate=self.config['training']['learning_rate'],
            weight_decay=self.config['training']['weight_decay'],
            patience=self.config['training']['patience'],
            model_name=model_name
        )
        
        # æµ‹è¯•æ¨¡å‹
        test_results = trainer.evaluate(
            test_loader=self.test_loader,
            model_path=self.output_dir / f'{model_name}_checkpoints' / f'{model_name}_best.pth'
        )
        
        total_time = time.time() - start_time
        
        # æ”¶é›†ç»“æœ
        experiment_results = {
            'model_name': model_name,
            'train_results': train_results,
            'test_results': test_results,
            'total_time': total_time,
            'data_info': self.data_info
        }
        
        if model_name == 'graphdiffusion':
            experiment_results['pretrain_results'] = pretrain_results
        
        # ä¿å­˜ç»“æœ
        self.save_experiment_results(model_name, experiment_results)
        
        # å…³é—­è®­ç»ƒå™¨
        trainer.close()
        
        print(f"\n{model_name.upper()} å®éªŒå®Œæˆ!")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {train_results['best_val_acc']:.4f}")
        print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_results['accuracy']:.4f}")
        print(f"æ€»ç”¨æ—¶: {total_time:.2f}s")
        
        return experiment_results
    
    def run_all_experiments(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ¨¡å‹å®éªŒ"""
        print("å¼€å§‹è¿è¡Œæ‰€æœ‰æ¨¡å‹å®éªŒ...")
        
        models_to_run = self.config['experiment']['models']
        
        for model_name in models_to_run:
            try:
                results = self.run_single_experiment(model_name)
                self.results[model_name] = results
            except Exception as e:
                print(f"âŒ {model_name.upper()} å®éªŒå¤±è´¥: {e}")
                self.results[model_name] = {'error': str(e)}
        
        # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
        self.generate_comparison_report()
        
        return self.results
    
    def save_experiment_results(self, model_name: str, results: Dict[str, Any]):
        """ä¿å­˜å•ä¸ªå®éªŒç»“æœ"""
        results_file = self.output_dir / f'{model_name}_results.json'
        
        # è½¬æ¢numpyç±»å‹ä¸ºPythonç±»å‹
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            else:
                return obj
        
        results = convert_numpy(results)
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
    
    def generate_comparison_report(self):
        """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
        print("\n" + "="*80)
        print("å®éªŒç»“æœæ¯”è¾ƒæŠ¥å‘Š")
        print("="*80)
        
        # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
        comparison_data = []
        
        for model_name, results in self.results.items():
            if 'error' in results:
                comparison_data.append({
                    'Model': model_name.upper(),
                    'Status': 'Failed',
                    'Error': results['error'],
                    'Val_Acc': 'N/A',
                    'Test_Acc': 'N/A',
                    'Test_F1': 'N/A',
                    'Test_AUC': 'N/A',
                    'Time(s)': 'N/A'
                })
            else:
                comparison_data.append({
                    'Model': model_name.upper(),
                    'Status': 'Success',
                    'Error': 'N/A',
                    'Val_Acc': f"{results['train_results']['best_val_acc']:.4f}",
                    'Test_Acc': f"{results['test_results']['accuracy']:.4f}",
                    'Test_F1': f"{results['test_results']['metrics']['f1']:.4f}",
                    'Test_AUC': f"{results['test_results']['metrics']['auc']:.4f}",
                    'Time(s)': f"{results['total_time']:.2f}"
                })
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(comparison_data)
        
        # ä¿å­˜ä¸ºCSV
        csv_file = self.output_dir / 'experiment_comparison.csv'
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # æ‰“å°è¡¨æ ¼
        print(df.to_string(index=False))
        
        # ä¿å­˜ä¸ºJSON
        json_file = self.output_dir / 'experiment_comparison.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False)
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        successful_results = {k: v for k, v in self.results.items() if 'error' not in v}
        if successful_results:
            best_model = max(successful_results.items(), 
                           key=lambda x: x[1]['test_results']['accuracy'])
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0].upper()}")
            print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_model[1]['test_results']['accuracy']:.4f}")
            print(f"   æµ‹è¯•F1åˆ†æ•°: {best_model[1]['test_results']['metrics']['f1']:.4f}")
            print(f"   æµ‹è¯•AUC: {best_model[1]['test_results']['metrics']['auc']:.4f}")
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")


def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config


def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    config = {
        "seed": 42,
        "data": {
            "root": "./data",
            "name": "HCPGender",
            "threshold": 0.1,
            "test_size": 0.2,
            "val_size": 0.1
        },
        "training": {
            "batch_size": 32,
            "num_epochs": 100,
            "pretrain_epochs": 50,
            "learning_rate": 0.001,
            "weight_decay": 1e-4,
            "patience": 20
        },
        "models": {
            "gcn": {
                "hidden_dims": [64, 32],
                "dropout": 0.5,
                "pool_type": "mean"
            },
            "graphsage": {
                "hidden_dims": [64, 32],
                "dropout": 0.5,
                "pool_type": "mean"
            },
            "gat": {
                "hidden_dims": [64, 32],
                "dropout": 0.5,
                "pool_type": "mean",
                "heads": [4, 4]
            },
            "graphvae": {
                "hidden_dims": [64, 32],
                "latent_dim": 128,
                "dropout": 0.5,
                "pool_type": "mean"
            },
            "graphdiffusion": {
                "hidden_dims": [64, 32],
                "dropout": 0.5,
                "pool_type": "mean",
                "num_timesteps": 1000
            }
        },
        "experiment": {
            "models": ["gcn", "graphsage", "gat", "graphvae", "graphdiffusion"]
        }
    }
    return config


def main():
    parser = argparse.ArgumentParser(description='è„‘ç½‘ç»œæ•°æ®é¢„æµ‹å®éªŒ')
    parser.add_argument('--config', type=str, default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset', type=str, default='HCPGender', 
                       choices=['HCPGender', 'HCPTask', 'HCPAge', 'HCPFI', 'HCPWM'],
                       help='è¦ä½¿ç”¨çš„NeuroGraphDatasetåç§°')
    parser.add_argument('--models', nargs='+', default=['gcn', 'graphsage', 'gat', 'graphvae', 'graphdiffusion'],
                       help='è¦è¿è¡Œçš„æ¨¡å‹åˆ—è¡¨')
    parser.add_argument('--output_dir', type=str, default='./experiments', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--create_config', action='store_true', help='åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
    if args.create_config:
        config = create_default_config()
        with open('config.json', 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print("é»˜è®¤é…ç½®æ–‡ä»¶å·²åˆ›å»º: config.json")
        return
    
    # åŠ è½½é…ç½®
    if Path(args.config).exists():
        config = load_config(args.config)
    else:
        print(f"é…ç½®æ–‡ä»¶ {args.config} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = create_default_config()
    
    # æ›´æ–°é…ç½®
    config['experiment']['models'] = args.models
    config['data']['name'] = args.dataset
    
    # åˆ›å»ºå®éªŒ
    experiment = BrainNetworkExperiment(
        config=config,
        output_dir=args.output_dir
    )
    
    # åŠ è½½æ•°æ®
    experiment.load_data()
    
    # è¿è¡Œå®éªŒ
    if len(args.models) == 1:
        # è¿è¡Œå•ä¸ªæ¨¡å‹
        results = experiment.run_single_experiment(args.models[0])
        experiment.results[args.models[0]] = results
    else:
        # è¿è¡Œæ‰€æœ‰æ¨¡å‹
        results = experiment.run_all_experiments()
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!")


if __name__ == "__main__":
    main()

